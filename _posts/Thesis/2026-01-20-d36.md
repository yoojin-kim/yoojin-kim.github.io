---
title: "[D-36] Fine-tuning (Line Completion)"  
categories: [Thesis]  
tags:
- [log, thesis]

toc: true
toc_sticky: true
date: 2026-01-20 21:00
---

ğŸ“‹ ì‘ì—… ìš”ì•½ (2026-01-20)
ğŸ¯ ëª©í‘œ
BiGSì™€ Mamba ëª¨ë¸ì„ CodeXGLUE line completion íƒœìŠ¤í¬ë¡œ fine-tuning
ğŸ”´ ë°œê²¬ëœ ë¬¸ì œë“¤

Dataset ë¬¸ì œ (Root Cause)

code_x_glue_cc_code_completion_line ë°ì´í„°ì…‹ì˜ gt í•„ë“œê°€ ì™„ì „íˆ ë¹„ì–´ìˆìŒ
ê²°ê³¼: loss=0.0, eval_loss=NaN


Context Truncation ë¬¸ì œ

ê¸´ ì½”ë“œê°€ max_length=512ë¡œ ì˜ë¦¬ë©´ì„œ target(gt)ì´ ì‚¬ë¼ì§
39%ì˜ ìƒ˜í”Œì—ì„œ valid labelsê°€ 0ê°œ


API í˜¸í™˜ì„± ë¬¸ì œ

HuggingFace Trainerì˜ log() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
start_time íŒŒë¼ë¯¸í„° í•„ìˆ˜í™”



âœ… í•´ê²° ë°©ë²•
ë°ì´í„°ì…‹ ì „í™˜:

code_x_glue_cc_code_completion_line (ë¬¸ì œ ìˆìŒ)
â†’ code_x_glue_cc_code_completion_token (ì •ìƒ)
100K train, 50K test samples
<EOL> í† í°ìœ¼ë¡œ ë¼ì¸ êµ¬ë¶„

ì „ì²˜ë¦¬ ë¡œì§ ê°œì„ :
python# â˜… í•µì‹¬: target ê¸¸ì´ë¥¼ ë¨¼ì € ê³„ì‚° í›„ context ì œí•œ
target_length = len(tokenizer(target_text)['input_ids'])
max_context_length = max(50, max_length - target_length - 10)
context_encoded = tokenizer(context_text, max_length=max_context_length)

# â˜… Validation: valid labels ì—†ìœ¼ë©´ skip
if sum(1 for l in labels if l != -100) == 0:
return {'input_ids': [], 'labels': []}
ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤
BiGS:

/mnt/user-data/outputs/finetune_line_bigs_fixed.py âœ…
/mnt/user-data/outputs/bigs-line-fixed-sbatch.sh âœ…
ì¶œë ¥ ê²½ë¡œ: /storage/athene/work/kim/causal/output_line_completion_fixed
ë¡œê·¸: line_bigs_fixed_output.txt, line_bigs_fixed_error.txt

Mamba:

/mnt/user-data/outputs/finetune_line_mamba_fixed.py âœ…
/mnt/user-data/outputs/mamba-line-fixed-sbatch.sh âœ…
ì¶œë ¥ ê²½ë¡œ: /storage/athene/work/kim/mamba/output_line_completion_fixed
ë¡œê·¸: line_mamba_fixed_output.txt, line_mamba_fixed_error.txt

ğŸ”§ ìµœì¢… ìˆ˜ì •ì‚¬í•­ (ì˜¤ëŠ˜ ì™„ë£Œ)

Triton Cache & WandB ì„¤ì • ì¶”ê°€ (ìŠ¤í¬ë¦½íŠ¸ ë§¨ ìœ„)

python   triton_cache = os.path.join(os.getcwd(), "triton_cache")
os.environ["XDG_CACHE_HOME"] = triton_cache
os.environ["TRITON_CACHE_DIR"] = triton_cache

log() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ìˆ˜ì •

python   def log(self, logs, start_time=None, **kwargs):
super().log(logs, start_time=start_time, **kwargs)
ğŸš€ ë‚´ì¼ í•  ì‘ì—…

íŒŒì¼ ë³µì‚¬ ë° ì‹¤í–‰

bash   # BiGS
cd /storage/athene/work/kim/causal
cp /mnt/user-data/outputs/finetune_line_bigs_fixed.py ./
sbatch bigs-line-fixed-sbatch.sh

# Mamba
cd /storage/athene/work/kim/mamba
cp /mnt/user-data/outputs/finetune_line_mamba_fixed.py ./
sbatch mamba-line-fixed-sbatch.sh

í•™ìŠµ ëª¨ë‹ˆí„°ë§

ë¡œê·¸ í™•ì¸: tail -f line_*_fixed_error.txt
í™•ì¸ì‚¬í•­:

âœ… Lossê°€ ì‹¤ì œ ê°’ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”ì§€
âœ… Eval lossê°€ NaNì´ ì•„ë‹Œì§€
âœ… Perplexity ê³„ì‚°ë˜ëŠ”ì§€
âœ… Accuracy ë©”íŠ¸ë¦­ ë‚˜ì˜¤ëŠ”ì§€




í•™ìŠµ ì™„ë£Œ í›„

ëª¨ë¸ ì €ì¥ í™•ì¸: output_line_completion_fixed/ ë””ë ‰í† ë¦¬
ìµœì¢… ì„±ëŠ¥ ë¹„êµ: BiGS vs Mamba



ğŸ“Œ ì°¸ê³ ì‚¬í•­

Pretrained ëª¨ë¸ ê²½ë¡œ:

BiGS: /storage/athene/work/kim/causal/output4/fim_code
Mamba: /storage/athene/work/kim/mamba/output_mamba/fim_code


ë°ì´í„°ì…‹ í¬ê¸°: 9000 train, 1000 eval
í•˜ì´í¼íŒŒë¼ë¯¸í„°:

BiGS LR: 5e-5
Mamba LR: 1e-4
Batch size: 8 (gradient accumulation: 4)
Epochs: 3
Max length: 512


ì´ì „ ì‹¤íŒ¨í•œ ì‹¤í–‰ë“¤: output_line_completion (not _fixed) ë””ë ‰í† ë¦¬ëŠ” ë¬´ì‹œí•˜ì…”ë„ ë©ë‹ˆë‹¤.

