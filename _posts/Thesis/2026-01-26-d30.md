---
title: "[D-30] Fine-tuning (Token Completion)"  
categories: [Thesis]  
tags:
- [log, thesis]

toc: true
toc_sticky: true
date: 2026-01-26 20:00
---
#Token-level Fine-tuning 
<==> Raw results;
```shell
<BiGS>
(base) [kim@slurm-login causal]$ cat token_completion_py150/test_results.json
{
  "eval_loss": 1.1523972749710083,
  "eval_accuracy": 0.727497134796791,
  "eval_runtime": 7.1555,
  "eval_samples_per_second": 13.975,
  "eval_steps_per_second": 1.817,
  "epoch": 5.0
}(base) [kim@slurm-login causal]$ cat token_completion_java/test_results.json
{
  "eval_loss": 1.3984194993972778,
  "eval_accuracy": 0.6893705287220082,
  "eval_runtime": 6.9239,
  "eval_samples_per_second": 14.443,
  "eval_steps_per_second": 1.878,
  "epoch": 5.0
}(base) [kim@slurm-login causal]$

<MAMBA>
(base) [kim@slurm-login mamba]$ cat token_completion_py150/test_results.json
{
  "eval_loss": 0.9129804968833923,
  "eval_accuracy": 0.7614387728114256,
  "eval_runtime": 6.0657,
  "eval_samples_per_second": 16.486,
  "eval_steps_per_second": 2.143,
  "epoch": 4.998736842105263
}(base) [kim@slurm-login mamba]$ cat token_completion_java/test_results.json
{
  "eval_loss": 1.0860016345977783,
  "eval_accuracy": 0.7338610676036067,
  "eval_runtime": 5.6439,
  "eval_samples_per_second": 17.718,
  "eval_steps_per_second": 2.303,
  "epoch": 5.0
}
```
ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­
1. Mambaê°€ BiGSë¥¼ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€

Python: 3.39% í¬ì¸íŠ¸ ì°¨ì´
Java: 4.45% í¬ì¸íŠ¸ ì°¨ì´
ë‘ ì–¸ì–´ ëª¨ë‘ì—ì„œ Mambaê°€ ëª…í™•í•œ ìš°ìœ„

2. Loss ê°’ ë¶„ì„

Mambaì˜ lossê°€ BiGSë³´ë‹¤ í˜„ì €íˆ ë‚®ìŒ (Python: 0.913 vs 1.152)
ë” ë‚˜ì€ ìˆ˜ë ´(convergence)ì„ ë³´ì—¬ì¤Œ

3. ì–¸ì–´ë³„ ì„±ëŠ¥

ë‘ ëª¨ë¸ ëª¨ë‘ Pythonì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥
Javaê°€ ë” ì–´ë ¤ìš´ taskì„ì„ ì‹œì‚¬

4. ì¶”ë¡  íš¨ìœ¨ì„±

Mambaê°€ ë” ë¹ ë¦„:

Python: 16.49 samples/sec (Mamba) vs 13.98 (BiGS)
Java: 17.72 samples/sec (Mamba) vs 14.44 (BiGS)


ì•½ 18-23% ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„


### Token-Level Code Completion

We evaluate both architectures on token-level code completion tasks
using the Py150 (Python) and Java benchmarks from CodeXGLUE.

**Results (Table X):**
- Mamba achieves 76.14% accuracy on Python, outperforming BiGS (72.75%)
  by 3.39 percentage points
- On Java, Mamba reaches 73.39% accuracy compared to BiGS's 68.94%,
  a 4.45 point improvement
- Mamba demonstrates 18-23% faster inference speed while maintaining
  superior accuracy

**Key Observations:**
1. Both models perform better on Python than Java, consistent with
   prior work showing Python's more regular syntax
2. Mamba's lower loss values (0.913 vs 1.152 on Python) indicate
   better convergence during training
3. The performance gap widens on Java, suggesting Mamba's selective
   state space mechanism better handles more complex syntax patterns
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

1. **Line Completion ì‹¤í—˜ ì§„í–‰**
   - ì´ë¯¸ ì¤€ë¹„ëœ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
   - Pythonê³¼ Java ëª¨ë‘ í‰ê°€

2. **ê²°ê³¼ í…Œì´ë¸” ì‘ì„±**
```
| Task | Language | Mamba | BiGS | CodeGPT* | Î” |
   |------|----------|-------|------|----------|---|
| Token | Python | 76.14 | 72.75 | 76.60 | -0.46 |
| Token | Java | 73.39 | 68.94 | - | - |
| Line | Python | ? | ? | - | - |
| Line | Java | ? | ? | - | - |
